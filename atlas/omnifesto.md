The planetary library requires a complete digital archive, multimodal models to access it, 
and collaborative tools to maintain it & ensure its knowledge integrity. All are in primordial stages.
To build it, we need an atlas of progress, gaps, & uses of this library – an OMN – that leaves no source unturned.
This will help map how we get to the shared tools we need to flourish, from the current disorganized, polarized, and commercially-intermediated state of public knowledge.  

While the Public Library reigns preeminent in public respect, and embodies the benefits of civilization and educational 
self-sufficiency, libraries have been under growing threat in recent decades. Repeated extension of copyright terms, 
restrictions on digitization, and a coordinated push by publishers to replace owning with renting (for books and software) 
has weakened libraries, while centralizing the control and monitoring of who reads what. 

At the dawn of the internet, digital reference desks were run by creative libraries. These were replaced by 
commercial search engines, 98% of which now aggressively track what users read, and sell placement in search results, 
degrading knowledge integrity and epistemic autonomy. Today there are only a few dozen digital libraries of significant size 
anywhere in the world, and many are under attack. The Internet Archive, the largest (and the only one archiving the Internet 
itself, our latest and greatest cultural megafact), is being vexatiously sued for $600M for archiving old records. 

At the same time, we have learned to train AI models on civilization-scale corpora to provide excellent recall and 
reference services — among other things. These are already essential tools for education, research, curation and synthesis. 
Access is again being provided by companies that mine usage data and are preparing to sell semantic similarity and salience / 
placement and persuasion to the highest bidder. Moreover, the foundation models used by most secondary tools and services 
are vulnerable to capture and control in new ways: training datasets and model weights can be massaged or poisoned to 
subtly rewrite history and to modify or ignore sources.

In an AI-augmented knowledge epoch, there is no substitute for a globally accessible, annotatable, public digital library, 
which preserves the cultural record in its original and unmodified context.  Such a library must encompass the canon of 
public knowledge on which models are trained, and come with its own privacy-preserving public models that anyone can use. 
It must be deployed in a robustly decent way, with each community able to provision and update its own library from both 
global and local collections. Finally, it must be collaboratively maintainable, letting communities self-organize archives, 
publications, and compute to advance the library.

As critical as this is to our civilization, we are far from it today. We need to work together to map and visualize the gaps, 
place them in context of the overall roadmap to a true digital library, and characterize the opportunity costs and 
switching costs entailed.  Done with care, this will provide context and motivation for every reader, curator, and planner to turn the page. 

To amplify Eco: the cultivated person's first duty is to be always prepared to recreate the universal library.
We must find ways to distribute the capture and backing up of our current physical and digital archives, as seen through the lens of both future readers and of knowledge models learning from the history of recorded thought; 
To build new interfaces to the best current tools and protocols designed to support 
decentralized curation, sharing, and storage at scale; 
To draw out new visualizations of what we want, so we can learn how to make it from what we have.
